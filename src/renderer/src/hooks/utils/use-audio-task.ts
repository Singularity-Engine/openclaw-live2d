/* eslint-disable func-names */
/* eslint-disable no-underscore-dangle */
/* eslint-disable @typescript-eslint/ban-ts-comment */
import { useRef, useEffect, useCallback } from 'react';
import { useTranslation } from 'react-i18next';
import { useAiState } from '@/context/ai-state-context';
import { useSubtitle } from '@/context/subtitle-context';
import { useChatHistory } from '@/context/chat-history-context';
import { audioTaskQueue } from '@/utils/task-queue';
import { audioManager } from '@/utils/audio-manager';
import { toaster } from '@/components/ui/toaster';
import { useWebSocket } from '@/context/websocket-context';
import { DisplayText } from '@/services/websocket-service';
import { useLive2DExpression } from '@/hooks/canvas/use-live2d-expression';
import * as LAppDefine from '../../../WebSDK/src/lappdefine';

// Simple type alias for Live2D model
type Live2DModel = any;

interface AudioTaskOptions {
  audioBase64: string
  volumes: number[]
  sliceLength: number
  displayText?: DisplayText | null
  expressions?: string[] | number[] | null
  speaker_uid?: string
  forwarded?: boolean
  audioFilePath?: string | null
  ttsEngineClass?: string | null
}

/**
 * Custom hook for handling audio playback tasks with Live2D lip sync
 */
export const useAudioTask = () => {
  const { t } = useTranslation();
  const { aiState, backendSynthComplete, setBackendSynthComplete } = useAiState();
  const { setSubtitleText } = useSubtitle();
  const { appendResponse, appendAIMessage } = useChatHistory();
  const { sendMessage } = useWebSocket();
  const { setExpression } = useLive2DExpression();

  // State refs to avoid stale closures
  const stateRef = useRef({
    aiState,
    setSubtitleText,
    appendResponse,
    appendAIMessage,
  });

  // Note: currentAudioRef and currentModelRef are now managed by the global audioManager

  stateRef.current = {
    aiState,
    setSubtitleText,
    appendResponse,
    appendAIMessage,
  };

  /**
   * Stop current audio playback and lip sync (delegates to global audioManager)
   */
  const stopCurrentAudioAndLipSync = useCallback(() => {
    audioManager.stopCurrentAudioAndLipSync();
  }, []);

  /**
   * Handle audio playback with Live2D lip sync
   */
  const handleAudioPlayback = (options: AudioTaskOptions): Promise<void> => new Promise((resolve) => {
    const {
      aiState: currentAiState,
      setSubtitleText: updateSubtitle,
      appendResponse: appendText,
      appendAIMessage: appendAI,
    } = stateRef.current;

    // Skip if already interrupted
    if (currentAiState === 'interrupted') {
      console.warn('Audio playback blocked by interruption state.');
      resolve();
      return;
    }

    const { audioBase64, displayText, expressions, forwarded, audioFilePath, ttsEngineClass } = options;

    // Update display text
    if (displayText) {
      appendText(displayText.text);
      appendAI(displayText.text, displayText.name, displayText.avatar);
      if (audioBase64) {
        updateSubtitle(displayText.text);
      }
      if (!forwarded) {
        sendMessage({
          type: "audio-play-start",
          display_text: displayText,
          forwarded: true,
        });
      }
    }

    try {
      // Process audio if available
      if (audioBase64) {
        const audioDataUrl = `data:audio/wav;base64,${audioBase64}`;

        // Get Live2D manager and model
        const live2dManager = (window as any).getLive2DManager?.();
        if (!live2dManager) {
          console.error('Live2D manager not found');
          resolve();
          return;
        }

        const model = live2dManager.getModel(0);
        if (!model) {
          console.error('Live2D model not found at index 0');
          resolve();
          return;
        }
        console.log('Found model for audio playback');

        if (!model._wavFileHandler) {
          console.warn('Model does not have _wavFileHandler for lip sync');
        } else {
          console.log('Model has _wavFileHandler available');
        }

        // Set expression if available
        const lappAdapter = (window as any).getLAppAdapter?.();
        if (lappAdapter && expressions?.[0] !== undefined) {
          setExpression(
            expressions[0],
            lappAdapter,
            `Set expression to: ${expressions[0]}`,
          );
        }

        // Start talk motion
        if (LAppDefine && LAppDefine.PriorityNormal) {
          console.log("Starting random 'Talk' motion");
          model.startRandomMotion(
            "Talk",
            LAppDefine.PriorityNormal,
          );
        } else {
          console.warn("LAppDefine.PriorityNormal not found - cannot start talk motion");
        }

        // Setup audio element
        const audio = new Audio(audioDataUrl);
        
        // Register with global audio manager IMMEDIATELY after creating audio
        audioManager.setCurrentAudio(audio, model);
        let isFinished = false;
        let timeoutId: NodeJS.Timeout;

        const cleanup = () => {
          console.log(`ðŸ§¹ [éŸ³é¢‘æ¸…ç†] å¼€å§‹æ¸…ç†éŸ³é¢‘ä»»åŠ¡: ${audioFilePath || 'unknown'}`);

          // é˜²æ­¢é‡å¤æ¸…ç†
          if (isFinished) {
            console.log('ðŸ§¹ [éŸ³é¢‘æ¸…ç†] ä»»åŠ¡å·²ç»æ¸…ç†è¿‡ï¼Œè·³è¿‡é‡å¤æ¸…ç†');
            return;
          }

          // æ¸…ç†éŸ³é¢‘å…ƒç´ 
          try {
            audio.pause();
            audio.src = '';
            audio.load();
          } catch (error) {
            console.warn('æ¸…ç†éŸ³é¢‘å…ƒç´ æ—¶å‡ºé”™:', error);
          }

          audioManager.clearCurrentAudio(audio);
          if (timeoutId) {
            clearTimeout(timeoutId);
          }

          // å¼ºåˆ¶æ£€æŸ¥å’Œè®°å½•éŸ³é¢‘ç®¡ç†å™¨çŠ¶æ€
          const hasCurrentAudio = audioManager.hasCurrentAudio();
          console.log(`ðŸ§¹ [éŸ³é¢‘æ¸…ç†] æ¸…ç†åŽéŸ³é¢‘ç®¡ç†å™¨çŠ¶æ€ - hasCurrentAudio: ${hasCurrentAudio}`);

          // åªæœ‰åœ¨éŸ³é¢‘æ­£å¸¸æ’­æ”¾å®Œæˆæ—¶æ‰å‘é€åˆ é™¤é€šçŸ¥ï¼ˆé¿å…è¶…æ—¶/é”™è¯¯æ—¶é‡å¤åˆ é™¤ï¼‰
          if (audioFilePath && !audio.error && audio.currentTime > 0) {
            try {
              const playbackCompleteMessage = {
                type: "frontend-playback-complete",
                audio_file_path: audioFilePath,
                tts_engine_class: ttsEngineClass,
                timestamp: Date.now(),
                playback_duration: audio.currentTime // æ·»åŠ æ’­æ”¾æ—¶é•¿éªŒè¯
              };
              sendMessage(playbackCompleteMessage);
              console.log(`ðŸ“¤ å·²å‘é€éŸ³é¢‘æ’­æ”¾å®Œæˆé€šçŸ¥: ${audioFilePath} (æ’­æ”¾æ—¶é•¿: ${audio.currentTime}s)`);
            } catch (error) {
              console.error('å‘é€éŸ³é¢‘æ’­æ”¾å®Œæˆé€šçŸ¥å¤±è´¥:', error);
              // ç§»é™¤é‡è¯•æœºåˆ¶ï¼Œé¿å…é‡å¤åˆ é™¤
            }
          } else if (audioFilePath) {
            console.warn(`âš ï¸ [éŸ³é¢‘æ–‡ä»¶è¿½è¸ª] éŸ³é¢‘å¯èƒ½æœªæ­£å¸¸æ’­æ”¾ï¼Œè·³è¿‡åˆ é™¤è¯·æ±‚: ${audioFilePath} (error: ${!!audio.error}, currentTime: ${audio.currentTime})`);
          }

          isFinished = true;
          resolve();
        };

        // æ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼Œé˜²æ­¢éŸ³é¢‘å¡æ­»
        timeoutId = setTimeout(() => {
          console.warn('Audio playback timeout, forcing cleanup');
          cleanup();
        }, 30000); // 30ç§’è¶…æ—¶

        // Enhance lip sync sensitivity
        const lipSyncScale = 2.0;

        audio.addEventListener('canplaythrough', () => {
          // Check for interruption before playback
          if (stateRef.current.aiState === 'interrupted' || !audioManager.hasCurrentAudio()) {
            console.warn('Audio playback cancelled due to interruption or audio was stopped');
            cleanup();
            return;
          }

          console.log('Starting audio playback with lip sync');
          audio.play().catch((err) => {
            console.error("Audio play error:", err);
            cleanup();
          });

          // Setup lip sync
          if (model._wavFileHandler) {
            if (!model._wavFileHandler._initialized) {
              console.log('Applying enhanced lip sync');
              model._wavFileHandler._initialized = true;

              const originalUpdate = model._wavFileHandler.update.bind(model._wavFileHandler);
              model._wavFileHandler.update = function (deltaTimeSeconds: number) {
                const result = originalUpdate(deltaTimeSeconds);
                // @ts-ignore
                this._lastRms = Math.min(2.0, this._lastRms * lipSyncScale);
                return result;
              };
            }

            if (audioManager.hasCurrentAudio()) {
              model._wavFileHandler.start(audioDataUrl);
            } else {
              console.warn('WavFileHandler start skipped - audio was stopped');
            }
          }
        });

        audio.addEventListener('ended', () => {
          console.log("Audio playback completed");
          cleanup();
        });

        audio.addEventListener('error', (error) => {
          console.error("Audio playback error:", error);
          cleanup();
        });

        audio.load();
      } else {
        resolve();
      }
    } catch (error) {
      console.error('Audio playback setup error:', error);
      toaster.create({
        title: `${t('error.audioPlayback')}: ${error}`,
        type: "error",
        duration: 2000,
      });
      resolve();
    }
  });

  // Handle backend synthesis completion
  useEffect(() => {
    let isMounted = true;
    let timeoutId: NodeJS.Timeout;

    const handleComplete = async () => {
      if (!backendSynthComplete) return;
      
      // ä½¿ç”¨è¶…æ—¶æœºåˆ¶é¿å…æ­»å¾ªçŽ¯ï¼Œæœ€å¤šç­‰å¾…10ç§’
      const waitForCompletionWithTimeout = () => {
        return new Promise<void>((resolve) => {
          const startTime = Date.now();
          const maxWaitTime = 10000; // 10ç§’è¶…æ—¶
          
          const checkCompletion = () => {
            if (!isMounted) {
              resolve();
              return;
            }
            
            if (!audioTaskQueue.hasTask()) {
              console.log('Audio queue completed normally');
              resolve();
              return;
            }
            
            if (Date.now() - startTime > maxWaitTime) {
              console.warn('Audio queue wait timeout, forcing completion');
              resolve();
              return;
            }
            
            timeoutId = setTimeout(checkCompletion, 200);
          };
          
          checkCompletion();
        });
      };
      
      await waitForCompletionWithTimeout();
      
      if (isMounted && backendSynthComplete) {
        stopCurrentAudioAndLipSync();
        // ç§»é™¤è¿™é‡Œçš„å…¨å±€åˆ é™¤è¯·æ±‚ï¼Œé¿å…é‡å¤åˆ é™¤
        // éŸ³é¢‘æ–‡ä»¶ä¼šåœ¨å„è‡ªçš„cleanupå‡½æ•°ä¸­åˆ é™¤
        // sendMessage({ type: "frontend-playback-complete" });
        setBackendSynthComplete(false);
      }
    };

    handleComplete();

    return () => {
      isMounted = false;
      if (timeoutId) {
        clearTimeout(timeoutId);
      }
    };
  }, [backendSynthComplete, sendMessage, setBackendSynthComplete, stopCurrentAudioAndLipSync]);

  /**
   * Add a new audio task to the queue
   */
  const addAudioTask = async (options: AudioTaskOptions) => {
    const { aiState: currentState } = stateRef.current;

    if (currentState === 'interrupted') {
      console.log('Skipping audio task due to interrupted state');
      return;
    }

    console.log(`Adding audio task ${options.displayText?.text} to queue`);
    audioTaskQueue.addTask(() => handleAudioPlayback(options));
  };

  return {
    addAudioTask,
    appendResponse,
    stopCurrentAudioAndLipSync,
  };
};
